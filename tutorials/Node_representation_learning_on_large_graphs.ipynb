{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Node representation learning on large graphs",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyyeh/citation-networks/blob/master/tutorials/Node_representation_learning_on_large_graphs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOvEGUfH9H5L",
        "colab_type": "text"
      },
      "source": [
        "# Node representation learning on large graphs\n",
        "\n",
        "In this tutorial, we will learn how to train unsupervised node embeddings in GraphVite. Node embeddings are critical to analyzing large graphs such as social networks and citation networks, especially when label data are inavailable or sparse.\n",
        "\n",
        "We will begin with training and evaluation steps of node embedding. Then we will show how to customize datasets and hyperparameters.\n",
        "\n",
        "---\n",
        "\n",
        "All the code here can be run on Google Colab directly, and results will be displayed in our browser. To run this tutorial,\n",
        "\n",
        "1. At the top-right of the menu bar, choose connect to hosted runtime.\n",
        "2. In the menu, choose Runtime -> Run all.\n",
        "\n",
        "Since Colab provides only 2 CPU threads and a very economic GPU, the code may take some time to run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN_GLRP_9O6z",
        "colab_type": "text"
      },
      "source": [
        "Download and install miniconda and GraphVite. This may take a while."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2tUFWvsmkn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -c https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!./Miniconda3-latest-Linux-x86_64.sh -b -p /usr/local -f\n",
        "\n",
        "!conda install -y -c milagraph -c conda-forge graphvite \\\n",
        "  python=3.6 cudatoolkit=$(nvcc -V | grep -Po \"(?<=V)\\d+\\.\\d+\")\n",
        "!conda install -y wurlitzer ipykernel\n",
        "\n",
        "import site\n",
        "site.addsitedir(\"/usr/local/lib/python3.6/site-packages\")\n",
        "%reload_ext wurlitzer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkRnJupT9cmt",
        "colab_type": "text"
      },
      "source": [
        "## Python Interface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKRw0-6KIIdH",
        "colab_type": "text"
      },
      "source": [
        "First, import the GraphVite package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aszDs0k9kVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import graphvite as gv\n",
        "import graphvite.application as gap"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxHveKrlIdyN",
        "colab_type": "text"
      },
      "source": [
        "Here we use a social network dataset, BlogCatalog, for demonstration. Each node in this graph is a blog user, and each edge represents a following relationship between two users.\n",
        "\n",
        "We specify the dimension of embedding vectors to be 128."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYljGoKy9kpb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "app = gap.GraphApplication(dim=128)\n",
        "app.load(file_name=gv.dataset.blogcatalog.train)\n",
        "app.build()\n",
        "app.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-HoEUUBJ8iD",
        "colab_type": "text"
      },
      "source": [
        "Once we learned the embeddings, we can evaluate them on a wide range of downstream tasks.\n",
        "\n",
        "Since some users are labeled with their interests in BlogCatalog, we validate whether our embeddings can be used for predicting those labels. We use 20% labeled data for training a linear classifier, and test on the rest data.\n",
        "\n",
        "The evaluation will report both macro-F1 and micro-F1 scores. Larger scores are better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9PYeJkW-Elk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "app.node_classification(file_name=gv.dataset.blogcatalog.label, portions=(0.2,))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp1r6DBwM0ca",
        "colab_type": "text"
      },
      "source": [
        "In some cases, we may not have node labels for our graphs. As an alternative, we can separate a small group edges from the training set, and evaluate on the link prediction task.\n",
        "\n",
        "To ensure that our model doesn't seen any test data, we should remove all training edges from the valid or test data.\n",
        "\n",
        "The performance is quantified by AUC, which is the area under the precision-recall curve. Larger AUC is better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztEPztZ4_pH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "app.link_prediction(file_name=gv.dataset.blogcatalog.valid,\n",
        "                    filter_file=gv.dataset.blogcatalog.train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1SKuMxdQdm7",
        "colab_type": "text"
      },
      "source": [
        "For our own graphs, we can customize a [`Dataset`](https://graphvite.io/docs/latest/api/dataset.html#graphvite.dataset.Dataset) and leverage the standard preprocess routines in GraphVite.\n",
        "\n",
        "For example, the following code defines a dataset that contains train, valid and test splits. The portions of positive edges in 3 splits are specified to be 2:1:1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcEYpa6__78Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyDataset(gv.dataset.Dataset):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(MyDataset, self).__init__(\n",
        "        \"my_dataset\",\n",
        "        urls={\"train\": [], \"valid\": [], \"test\": []}\n",
        "    )\n",
        "  \n",
        "  def train_preprocess(self, train_file):\n",
        "      valid_file = train_file[:train_file.rfind(\"train.txt\")] + \"valid.txt\"\n",
        "      test_file = train_file[:train_file.rfind(\"train.txt\")] + \"test.txt\"\n",
        "      # change the graph to our own path\n",
        "      self.link_prediction_split(gv.dataset.blogcatalog.graph,\n",
        "                                 [train_file, valid_file, test_file],\n",
        "                                 portions=[2, 1, 1])\n",
        "\n",
        "my_dataset = MyDataset()\n",
        "\n",
        "with open(my_dataset.train, \"r\") as fin:\n",
        "  print(\"my_dataset.train: #positive: %d\" % len(fin.readlines()))\n",
        "count = [0] * 2\n",
        "with open(my_dataset.valid, \"r\") as fin:\n",
        "  for line in fin:\n",
        "    count[int(line.strip()[-1])] += 1\n",
        "  print(\"my_dataset.valid: #negative: %d, #positive: %d\" % tuple(count))\n",
        "count = [0] * 2\n",
        "with open(my_dataset.test, \"r\") as fin:\n",
        "  for line in fin:\n",
        "    count[int(line.strip()[-1])] += 1\n",
        "  print(\"my_dataset.test: #negative: %d, #positive: %d\" % tuple(count))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "op9g7PkBUfLU",
        "colab_type": "text"
      },
      "source": [
        "The learned embeddings can be accessed through"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5h-U9rhB2sf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "name2id = app.graph.name2id\n",
        "vertex_embeddings = app.solver.vertex_embeddings\n",
        "context_embeddings = app.solver.context_embeddings\n",
        "\n",
        "# Get the vertex embedding of node \"1\"\n",
        "print(vertex_embeddings[name2id[\"1\"]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gM_8bI8QUmKw",
        "colab_type": "text"
      },
      "source": [
        "Or we can save the learned embeddings by"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXXEPVEPC4XX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "app.save_model(\"line_blogcatalog.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUIzU0nCDFWR",
        "colab_type": "text"
      },
      "source": [
        "## Configuration File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRrCCcJnDHpl",
        "colab_type": "text"
      },
      "source": [
        "GraphVite supports a configuration file interface, which can simplify our training procedure.\n",
        "\n",
        "We can create a configuration scaffold with the following line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPDbXdfsDJvh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!graphvite new graph --file my_config.yaml\n",
        "# node (word) embedding for natural language corpus\n",
        "!graphvite new word graph --file word_graph.yaml"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLoCs0PUDJDZ",
        "colab_type": "text"
      },
      "source": [
        "In the left pane, we can find `my_config.yaml` in *Files* tab. As Colab doesn't support editing very well, we can edit it in the following cell and then run the cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpzjbADmDKYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile my_config.yaml\n",
        "application:\n",
        "  graph\n",
        "\n",
        "resource:\n",
        "  # List of GPU ids. Default is all GPUs\n",
        "  gpus: []\n",
        "  # Memory limit for each GPU in bytes. Default is all available memory.\n",
        "  gpu_memory_limit: auto\n",
        "  # Number of CPU thread per GPU. Default is all CPUs.\n",
        "  cpu_per_gpu: auto\n",
        "  # Dimension of the embeddings.\n",
        "  dim: 128\n",
        "\n",
        "format:\n",
        "  # String of delimiter characters. Change it if your node name contains blank character.\n",
        "  delimiters: \" \\t\\r\\n\"\n",
        "  # Prefix of comment strings. Change it if you use comment style other than Python.\n",
        "  comment: \"#\"\n",
        "\n",
        "graph:\n",
        "  # Path to edge list file. Each line should be one of the following\n",
        "  # [node 1] [delimiter] [node 2] [comment]...\n",
        "  # [node 1] [delimiter] [node 2] [delimiter] [weight] [comment]...\n",
        "  # [comment]...\n",
        "  # For standard datasets, you can specify them by <[dataset].[split]>.\n",
        "  file_name:\n",
        "  # Symmetrize the graph or not. True is recommended.\n",
        "  as_undirected: true\n",
        "  # Normalize the adjacency matrix or not. This may influence the performance a little.\n",
        "  normalization: false\n",
        "\n",
        "build:\n",
        "  optimizer:\n",
        "    # Optimizer.\n",
        "    type: SGD\n",
        "    # Learning rate. Default is usually reasonable.\n",
        "    lr: 0.025\n",
        "    # Weight decay.\n",
        "    weight_decay: 0.005\n",
        "    # Learning rate schedule, can be \"linear\" or \"constant\". Linear is recommended.\n",
        "    schedule: linear\n",
        "  # Number of partitions. Auto is recommended.\n",
        "  num_partition: auto\n",
        "  # Number of negative samples per positive sample.\n",
        "  # Larger value results in slower training.\n",
        "  # The performance may be influenced by num_negative * negative_weight.\n",
        "  num_negative: 1\n",
        "  # Batch size of samples in CPU-GPU transfer. Default is recommended.\n",
        "  batch_size: 100000\n",
        "  # Number of batches in a partition block.\n",
        "  # Default is recommended.\n",
        "  episode_size: auto\n",
        "\n",
        "# Comment out this section if not needed.\n",
        "load:\n",
        "  # Path to model file, can be \"*.pkl\".\n",
        "  file_name: graph.pkl\n",
        "\n",
        "train:\n",
        "  # Model, can be DeepWalk, LINE or node2vec.\n",
        "  model: DeepWalk\n",
        "  # Number of epochs. Default is usually reasonable for sparse graphs.\n",
        "  # For dense graphs (|E| / |V| > 100), you may use smaller values.\n",
        "  num_epoch: 2000\n",
        "  # Resume training from a loaded model.\n",
        "  resume: false\n",
        "  # Weight of negative samples. Values larger than 10 may cause unstable training.\n",
        "  negative_weight: 5\n",
        "  # Exponent of degrees in negative sampling. Default is recommended.\n",
        "  negative_sample_exponent: 0.75\n",
        "  # Augmentation step. Default is usually reasonable.\n",
        "  # Larger value is needed for sparser graphs.\n",
        "  augmentation_step: auto\n",
        "  # Return parameter and in-out parameters (node2vec). Need to be tuned on the validation set.\n",
        "  p: 1\n",
        "  q: 1\n",
        "  # Length of each random walk. Default is recommended.\n",
        "  random_walk_length: 40\n",
        "  # Batch size of random walks in samplers. Default is recommended.\n",
        "  random_walk_batch_size: 100\n",
        "  # Log every n batches.\n",
        "  log_frequency: 1000\n",
        "\n",
        "# Comment out this section if not needed.\n",
        "evaluate:\n",
        "  # Comment out any task if not needed.\n",
        "  - task: node classification\n",
        "    # Path to node label file. Each line should be one of the following\n",
        "    # [node] [delimiter] [label] [comment]...\n",
        "    # [comment]...\n",
        "    file_name:\n",
        "    # Portions of data used for training. Each of them corresponds to one evaluation.\n",
        "    portions: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "    # Number of trials repeated. Change it to 1 if your evaluation set is large enough.\n",
        "    times: 5\n",
        "\n",
        "  - task: link prediction\n",
        "    # Path to link prediction file. Each line should be\n",
        "    # [node 1] [delimiter] [node 2] [delimiter] [label]\n",
        "    # where label is 1 for positive and 0 for negative.\n",
        "    file_name:\n",
        "    # Path to filter file. If you aren't sure that training data is excluded in evaluation,\n",
        "    # you can specify the training edge list here.\n",
        "    filter_file:\n",
        "\n",
        "# Comment out this section if not needed.\n",
        "save:\n",
        "  # Path to save file, can be \"*.pkl\".\n",
        "  file_name: graph.pkl\n",
        "  # Save hyperparameters or not.\n",
        "  save_hyperparameter: false"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEhdAlOeV-J0",
        "colab_type": "text"
      },
      "source": [
        "To run our configuration file, use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_7mNDvEDi29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !graphvite run my_config.yaml --cpu 2 --gpu 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MV9W4DwWo2W",
        "colab_type": "text"
      },
      "source": [
        "We can also check out existing benchmarks in GraphVite. The full list of benchmarks can be found [here](https://graphvite.io/docs/latest/benchmark.html#node-embedding).\n",
        "\n",
        "Here we train LINE on Youtube dataset, which is a million-scale social network. Due to time reasons, we skip the evaluation step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjCoPmZCDiwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!graphvite baseline line youtube --epoch 200 --no-eval"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POVhwjf0DqvF",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6b1uvo9ZgoR",
        "colab_type": "text"
      },
      "source": [
        "The default hyperparameters of node embeddings are usually robust. If we want to get the best performance, we may tune the following hyperparmeters, ranked by decreasing importance.\n",
        "\n",
        "1. `augmentation_step`. It determines the length of random walk augmentation. Generally, we should use a larger value for sparse graphs. Common values are 1, 2, 5 and 10.\n",
        "\n",
        "2. `num_negative` and `negative_weight`. They control the negative sampling strategy. Common values are 1, 5, 10 for `num_negative` and 1, 2, 5 for `negative_weight`.\n",
        "\n",
        "3. For node2vec, `p` and `q` control the biased random walk. Common values are 0.25, 1 and 4."
      ]
    }
  ]
}